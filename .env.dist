PYTHONPATH=./src

# Hugging Face settings
HF_TOKEN=whatever
HF_REPO_NAME=your_name/lora_model

# Ollama Settings
OLLAMA_HOST=http://localhost:11434

# Model settings
BASE_MODEL=unsloth/Llama-3.1-8B-unsloth-bnb-4bit

# Dataset Settings
DATASET_NAME=gretelai/synthetic_text_to_sql

# Model Settings
MAX_SEQ_LENGTH=2048
LORA_RANK=16
LOAD_IN_4BIT=true

# Peft Settings
LORA_RANK=16
LORA_ALPHA=16
LORA_DROPOUT=0
BIAS=none
USE_GRADIENT_CHECKPOINTING=unsloth
RANDOM_STATE=3407
USE_RSLORA=false

# Trainer Settings
PER_DEVICE_TRAIN_BATCH_SIZE=2
GRADIENT_ACCUMULATION_STEPS=4
WARMUP_STEPS=5
MAX_STEPS=60
LEARNING_RATE=2e-4
LOGGING_STEPS=1
OPTIMIZER=adamw_8bit
WEIGHT_DECAY=0.01
LR_SCHEDULER_TYPE=linear
SEED=3407
